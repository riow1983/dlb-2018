{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. Deep Q-Network（DQN）でMountainCarを攻略せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-Network（DQN）により、MountainCarを攻略してみましょう。  \n",
    "今回の評価は提出点のみとなりますが、より上手にゲームを攻略できるようチャレンジしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ルール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "環境としてMountainCar-v0を利用します。  \n",
    "MountainCarは二つの山の間にある車を右の山の頂上まで運ぶゲームです。  \n",
    "エピソード終了時のRewardが-200よりも大きくなれば、成功となります。  \n",
    "- state: サイズ(2,)のnp.ndarray\n",
    "  - (車の位置, 車の速度)\n",
    "- action:\n",
    "    - 0: 車を左に移動させる\n",
    "    - 1: 車を移動させない\n",
    "    - 2: 車を右に移動させる\n",
    "- reward:\n",
    "    -1: エピソード終了まで\n",
    "- terminal:\n",
    "    - False: エピソード継続\n",
    "    - True: エピソード終了 (ゴールするか、200step経過)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MountainCarをDQNによって攻略するコードをsubmission_code.pyとして提出してください。(%%writefileコマンドなどを利用してください)。\n",
    "- なお今回の評価は提出点のみとなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプルコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -200.0, Q_max: -2.0331, eps: 0.8201\n",
      "Episode: 20, Reward: -200.0, Q_max: -3.8923, eps: 0.6401\n",
      "Episode: 30, Reward: -200.0, Q_max: -5.7684, eps: 0.4601\n",
      "Episode: 40, Reward: -200.0, Q_max: -7.6104, eps: 0.2801\n",
      "Episode: 50, Reward: -200.0, Q_max: -9.2198, eps: 0.1001\n",
      "Episode: 60, Reward: -200.0, Q_max: -10.8873, eps: 0.1000\n",
      "Episode: 70, Reward: -200.0, Q_max: -12.4802, eps: 0.1000\n",
      "Episode: 80, Reward: -200.0, Q_max: -13.8907, eps: 0.1000\n",
      "Episode: 90, Reward: -200.0, Q_max: -15.4099, eps: 0.1000\n",
      "Episode: 100, Reward: -200.0, Q_max: -16.6798, eps: 0.1000\n",
      "Episode: 110, Reward: -200.0, Q_max: -17.9851, eps: 0.1000\n",
      "Episode: 120, Reward: -200.0, Q_max: -19.1867, eps: 0.1000\n",
      "Episode: 130, Reward: -200.0, Q_max: -20.4716, eps: 0.1000\n",
      "Episode: 140, Reward: -200.0, Q_max: -21.7175, eps: 0.1000\n",
      "Episode: 150, Reward: -200.0, Q_max: -22.4386, eps: 0.1000\n",
      "Episode: 160, Reward: -200.0, Q_max: -23.8991, eps: 0.1000\n",
      "Episode: 170, Reward: -200.0, Q_max: -25.6408, eps: 0.1000\n",
      "Episode: 180, Reward: -200.0, Q_max: -26.6737, eps: 0.1000\n",
      "Episode: 190, Reward: -200.0, Q_max: -27.6805, eps: 0.1000\n",
      "Episode: 200, Reward: -200.0, Q_max: -28.2418, eps: 0.1000\n",
      "Episode: 210, Reward: -200.0, Q_max: -28.4260, eps: 0.1000\n",
      "Episode: 220, Reward: -200.0, Q_max: -29.1874, eps: 0.1000\n",
      "Episode: 230, Reward: -200.0, Q_max: -4.2855, eps: 0.1000\n",
      "Episode: 240, Reward: -200.0, Q_max: -31.3822, eps: 0.1000\n",
      "Episode: 250, Reward: -200.0, Q_max: -32.4142, eps: 0.1000\n"
     ]
    }
   ],
   "source": [
    "#%%writefile /root/userspace/chap10/materials/submission_code.py\n",
    "\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# WRITE ME\n",
    "######## Q Network and Target Network --------------------------------------------------------------\n",
    "\n",
    "n_states = 2\n",
    "n_actions = 3\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "x_state = tf.placeholder(tf.float32, [None, n_states])\n",
    "\n",
    "def original_network(x):\n",
    "    with tf.variable_scope('Original', reuse=tf.AUTO_REUSE):\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)# WRITE ME    # kernel_initializer=initializer\n",
    "    return y\n",
    "\n",
    "def target_network(x):\n",
    "    with tf.variable_scope('Target', reuse=tf.AUTO_REUSE):\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)# WRITE ME    # activation = tf.nn.elu, kernel_initializer=initializer\n",
    "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)# WRITE ME    # kernel_initializer=initializer\n",
    "    return y\n",
    "\n",
    "\n",
    "q_original = original_network(x_state)\n",
    "vars_original = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Original')\n",
    "\n",
    "q_target = target_network(x_state)\n",
    "vars_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Target')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## Copy the weight of network to Target Network ----------------------------------------------\n",
    "\n",
    "copy_ops = [var_target.assign(var_original) for var_target, var_original in zip(vars_target, vars_original)]\n",
    "copy_weights = tf.group(*copy_ops)\n",
    "\n",
    "\n",
    "######## Training operation ------------------------------------------------------------------------\n",
    "\n",
    "t = tf.placeholder(tf.float32, [None])\n",
    "x_action = tf.placeholder(tf.int32, [None])\n",
    "q_value = tf.reduce_sum(q_original * tf.one_hot(x_action, n_actions), axis=1)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.subtract(t,q_value)))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_ops = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "######## Experience Replay -------------------------------------------------------------------------\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque([], maxlen = memory_size)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size).tolist()\n",
    "\n",
    "        state      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
    "        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
    "        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
    "        action     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
    "        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])\n",
    "        \n",
    "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}\n",
    "    \n",
    "## record the history of random actions onto ReplayMemory in advance before starting the training\n",
    "memory_size = 50000 # memory size\n",
    "initial_memory_size = 500 # the number of experience in advance \n",
    "\n",
    "\n",
    "replay_memory = ReplayMemory(memory_size)\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        action = env.action_space.sample() # WRITE ME #ランダムに行動を選択\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action) # WRITE ME #状態、報酬、終了判定の取得\n",
    "        \n",
    "        transition = {\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition) #経験の記憶\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    if step >= initial_memory_size:\n",
    "        break\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "######## Train -------------------------------------------------------------------------------------\n",
    "\n",
    "## epsilon-greedy policy\n",
    "eps_start = 1.0\n",
    "eps_end = 0.1\n",
    "n_steps = 10000\n",
    "def get_eps(step):\n",
    "    return max(0.1, (eps_end - eps_start) / n_steps * step + eps_start)\n",
    "\n",
    "## define hyperparameters\n",
    "gamma = 0.99\n",
    "target_update_interval = 1000 #重みの更新間隔\n",
    "batch_size = 32\n",
    "n_episodes = 300\n",
    "step = 0\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## start training for n_episodes times\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    copy_weights.run() #初期重みのコピー\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        terminal = False\n",
    "\n",
    "        total_reward = 0\n",
    "        total_q_max = []\n",
    "        while not terminal:\n",
    "            q = q_original.eval(feed_dict={x_state: state[None]}) #Q値の計算\n",
    "            total_q_max.append(np.max(q))\n",
    "\n",
    "            eps = get_eps(step) #εの更新\n",
    "            if np.random.random() < eps:\n",
    "                action = env.action_space.sample() #（ランダムに）行動を選択\n",
    "            else:\n",
    "                action = np.argmax(q) #行動を選択\n",
    "            next_state, reward, terminal, _ = env.step(action) #状態、報酬、終了判定の取得\n",
    "            reward = np.sign(reward) # clipping\n",
    "            total_reward += reward #エピソード内の報酬を更新\n",
    "\n",
    "            transition = {\n",
    "                'state': state,\n",
    "                'next_state': next_state,\n",
    "                'reward': reward,\n",
    "                'action': action,\n",
    "                'terminal': int(terminal)\n",
    "            }\n",
    "            replay_memory.append(transition) #経験の記憶\n",
    "            \n",
    "            batch = replay_memory.sample(batch_size) #経験のサンプリング\n",
    "            q_target_next = q_target.eval(feed_dict={x_state: batch['next_state']}) #ターゲットQ値の計算\n",
    "            t_value = batch['reward'] + (1 - batch['terminal']) * gamma * q_target_next.max(1)\n",
    "            \n",
    "            train_ops.run(feed_dict = {x_state: batch['state'], x_action: batch['action'], t: t_value}) #訓練オペレーション\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            env.render() #画面の出力\n",
    "\n",
    "            if (step + 1) % target_update_interval == 0:\n",
    "                copy_weights.run() #一定期間ごとに重みをコピー\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}'.format(episode + 1, total_reward, np.mean(total_q_max), eps))\n",
    "\n",
    "    # 学習させたネットワークでTest\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "\n",
    "    total_reward = 0\n",
    "    while not terminal:\n",
    "        q = q_original.eval(feed_dict={x_state: state[None]})\n",
    "        action = np.argmax(q)\n",
    "\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    print('Test Reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
