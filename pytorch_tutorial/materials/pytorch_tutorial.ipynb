{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1回講義 演習 PyTorchの基礎を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本演習では、以下の構成に従って、PyTorchの基礎を学んでいきます。\n",
    "\n",
    "1. PyTorchとは\n",
    "    - 1.1 PyTorchとは\n",
    "    - 1.2 ライブラリの構成\n",
    "2. Tensor\n",
    "    - 2.1　基本的な行列\n",
    "    - 2.2 型の定義\n",
    "    - 2.3 サイズの確認\n",
    "    - 2.4 変形\n",
    "    - 2.5 演算\n",
    "    - 2.6 Numpy, list, scalarへの変換\n",
    "    - 2.7 デバイス(CPU/CUDA)の指定\n",
    "3. Autograd\n",
    "4. nn\n",
    "    - 4.1 Module\n",
    "    - 4.2 活性化関数\n",
    "    - 4.3 誤差関数\n",
    "    - 4.4 モジュール化\n",
    "    - 4.5 最適化\n",
    "    - 4.6 学習\n",
    "    - 4.7 モデルの保存・読み込み・再学習\n",
    "5. Tips:torchvision & DataLoader\n",
    "5. 課題：PyTorchを使ってMLPを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorchとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. PyTorchとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchは, 主にFacebookによって開発されているニューラルネットワーク用のライブラリです.\n",
    "TensorFlowやTheanoでは, ネットワークの計算グラフを定義した後にコンパイルしてからデータを流す (静的フレームワーク) のに対し, PyTorchではデータが流れるごとに計算グラフを動的に構築する (動的フレームワーク) ため, データごとにグラフの形状が異なる自然言語処理などに向いているとされています."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式ドキュメント: https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ライブラリの構成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figures/pytorch_overview.png\" width=\"600mm\">\n",
    "\n",
    "出典: PyTorch公式 (https://pytorch.org/about/)\n",
    "\n",
    "出典: PyTorchのススメ (https://www.slideshare.net/yuyasoneoka/pytorch-80883065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorは、変数をグラフ上のノードとして表すためのクラスです。\n",
    "\n",
    "NumPyのndarrayに似たAPIのまま、GPU上での高速計算（CUDA）を扱えるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1 基本的な行列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "基本的な行列は、numpyと同様の関数を使って作ることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# torch.ones:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "# torch.zeros:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "# torch.full:\n",
      "tensor([[99., 99., 99.],\n",
      "        [99., 99., 99.]])\n",
      "\n",
      "# torch.eye:\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 全要素が1の行列 (numpy.ones)\n",
    "a = torch.ones((2, 3))\n",
    "print(\"# torch.ones:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 全要素が0の行列 (numpy.ones)\n",
    "a = torch.zeros((2, 3))\n",
    "print(\"# torch.zeros:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 指定した値で満たされた行列 (numpy.full)\n",
    "a = torch.full((2, 3), fill_value=99)\n",
    "print(\"# torch.full:\")\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# 単位行列 (numpy.eye)\n",
    "a = torch.eye(2)\n",
    "print(\"# torch.eye:\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "一般的な乱数は大体揃っています. シードは`torch.manual_seed`で指定できます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# torch.randn（正規分布）:\n",
      "tensor([ 1.4219, -0.2314])\n",
      "\n",
      "# torch.rand（一様分布）:\n",
      "tensor([0.7322, 0.0119])\n",
      "\n",
      "# 確率p:\n",
      "tensor([[0.0205, 0.6273, 0.5561],\n",
      "        [0.9923, 0.6566, 0.8479]])\n",
      "\n",
      "# torch.bernoulli（ベルヌーイ分布）:\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "# torch.multinominal（多項分布）:\n",
      "tensor([1, 1, 0, 1, 1, 2, 2, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "# 標準正規分布 (numpy.random.randn)\n",
    "print(\"# torch.randn（正規分布）:\")\n",
    "print(torch.randn(2))\n",
    "print()\n",
    "\n",
    "# [0, 1)の一様分布 (numpy.random.rand)\n",
    "print(\"# torch.rand（一様分布）:\")\n",
    "print(torch.rand(2))\n",
    "print()\n",
    "\n",
    "# ベルヌーイ分布\n",
    "probs = torch.rand((2, 3))\n",
    "print(\"# 確率p:\")\n",
    "print(probs)\n",
    "print()\n",
    "print(\"# torch.bernoulli（ベルヌーイ分布）:\")\n",
    "print(torch.bernoulli(probs))\n",
    "print()\n",
    "\n",
    "# 多項分布 (np.random.multinomial)\n",
    "probs = torch.tensor([0.2, 0.4, 0.4])\n",
    "print(\"# torch.multinominal（多項分布）:\")\n",
    "print(torch.multinomial(probs, num_samples=10, replacement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.2 型の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "dtypeの引数を指定することで、Tensorの型を定義することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "torch.int32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "torch.int64\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.float)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3), dtype=torch.int)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3), dtype=torch.long)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.3 サイズの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`.size()` を用いることで、Tensorのサイズを確認することができます。\n",
    "\n",
    "(`.size()`のエイリアスとして`.shape`も存在するので、numpyのように`.shape`で取得することも可能です。)\n",
    "\n",
    "\n",
    "引数を指定することで特定の次元のサイズのみを取得することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3))\n",
    "\n",
    "print(a.size())  # Tensorのサイズを取得\n",
    "print(a.shape)  # .shapeでも可能\n",
    "\n",
    "print(a.size(0))  # Tensorの0次元目のサイズを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 変形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの変形も同様に行えます。\n",
    "\n",
    "軸方向を指定する引数名はnumpyではaxisでしたが、PyTorchではdimであることに注意する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.1 次元の追加・除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "torch.Size([6])\n",
      "\n",
      "# reshape(2,3):\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "# view(1,2,3,1):\n",
      "tensor([[[[0],\n",
      "          [1],\n",
      "          [2]],\n",
      "\n",
      "         [[3],\n",
      "          [4],\n",
      "          [5]]]])\n",
      "torch.Size([1, 2, 3, 1])\n",
      "\n",
      "tensor([[[[0],\n",
      "          [1],\n",
      "          [2]],\n",
      "\n",
      "         [[3],\n",
      "          [4],\n",
      "          [5]]]])\n",
      "\n",
      "# view(-1,2)で最後のdimのsizeを2に, 他のdimは潰す:\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "torch.Size([3, 2])\n",
      "\n",
      "# unsqueeze(dim=1)で次元をdim1に追加:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n",
      "torch.Size([6, 1])\n",
      "\n",
      "# squeeze()でサイズ1の次元を削除:\n",
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "torch.Size([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6)  # 等差数列を作成 (numpy.arange)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "b = a.reshape(2, 3)  # 変形 (numpy.reshape)\n",
    "print(\"# reshape(2,3):\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.view(1, 2, 3, 1) # dimの追加も可能\n",
    "print(\"# view(1,2,3,1):\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# a.viewでエラーが出ることがある場合は.contiguous()を呼んでからview()する (参照: https://discuss.pytorch.org/t/runtimeerror-input-is-not-contiguous/930)\n",
    "print(a.contiguous().view(1, 2, 3, 1))\n",
    "print()\n",
    "\n",
    "b = a.view(-1, 2) # 最後のdimのsizeを2に, 他のdimは\"潰す\"\n",
    "print(\"# view(-1,2)で最後のdimのsizeを2に, 他のdimは潰す:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.unsqueeze(dim=1)  # 新しいdimを追加する。 a[:, None]でもOK (numpy.expand_dims)\n",
    "print(\"# unsqueeze(dim=1)で次元をdim1に追加:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "c = b.squeeze()  # squeezeを用いるとsizeが1の次元が除去される (numpy.squeeze)\n",
    "print(\"# squeeze()でサイズ1の次元を削除:\")\n",
    "print(c)\n",
    "print(c.size())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.2 次元の入れ替え・変形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]]])\n",
      "torch.Size([1, 2, 3])\n",
      "\n",
      "# transpose(0, 2)でdim0とdim2の次元を入れ替える:\n",
      "tensor([[[0],\n",
      "         [3]],\n",
      "\n",
      "        [[1],\n",
      "         [4]],\n",
      "\n",
      "        [[2],\n",
      "         [5]]])\n",
      "torch.Size([3, 2, 1])\n",
      "\n",
      "# permute(0, 2, 1)で順番を並び替える:\n",
      "tensor([[[0, 3],\n",
      "         [1, 4],\n",
      "         [2, 5]]])\n",
      "torch.Size([1, 3, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(1, 2, 3)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "b = a.transpose(0, 2)  # dim0とdim2の次元を入れ替える (numpy.transpose)\n",
    "print(\"# transpose(0, 2)でdim0とdim2の次元を入れ替える:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "b = a.permute(0, 2, 1)  # 指定したdimの順に次元を入れ替える\n",
    "print(\"# permute(0, 2, 1)で順番を並び替える:\")\n",
    "print(b) \n",
    "print(b.size())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2.4.3 分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`torch.split`は指定したサイズでテンソルの分割を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "\n",
      "# torch.splitで2個ずつに分割:\n",
      "(tensor([[0, 1],\n",
      "        [5, 6]]), tensor([[2, 3],\n",
      "        [7, 8]]), tensor([[4],\n",
      "        [9]]))\n",
      "[torch.Size([2, 2]), torch.Size([2, 2]), torch.Size([2, 1])]\n",
      "\n",
      "# torch.splitで1,3,1個ずつに分割:\n",
      "(tensor([[0],\n",
      "        [5]]), tensor([[1, 2, 3],\n",
      "        [6, 7, 8]]), tensor([[4],\n",
      "        [9]]))\n",
      "[torch.Size([2, 1]), torch.Size([2, 3]), torch.Size([2, 1])]\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "b = torch.split(a, 2, dim=1)  # 2要素ごとに分割(割り切れない場合は最後の要素が余りになる) (numpy.split)\n",
    "print(\"# torch.splitで2個ずつに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()\n",
    "\n",
    "b = torch.split(a, [1, 3, 1], dim=1)  # リストを指定するとそのサイズで分割\n",
    "print(\"# torch.splitで1,3,1個ずつに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`torch.chunk`は指定した個数にテンソルを分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "\n",
      "# chunkで5グループに分割:\n",
      "(tensor([[0],\n",
      "        [5]]), tensor([[1],\n",
      "        [6]]), tensor([[2],\n",
      "        [7]]), tensor([[3],\n",
      "        [8]]), tensor([[4],\n",
      "        [9]]))\n",
      "[torch.Size([2, 1]), torch.Size([2, 1]), torch.Size([2, 1]), torch.Size([2, 1]), torch.Size([2, 1])]\n",
      "\n",
      "# chunkで3グループに分割:\n",
      "(tensor([[0, 1],\n",
      "        [5, 6]]), tensor([[2, 3],\n",
      "        [7, 8]]), tensor([[4],\n",
      "        [9]]))\n",
      "[torch.Size([2, 2]), torch.Size([2, 2]), torch.Size([2, 1])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "b = torch.chunk(a, 5, dim=1) # 5グループに分割\n",
    "print(\"# chunkで5グループに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()\n",
    "\n",
    "b = torch.chunk(a, 3, dim=1) # 3グループに分割(割り切れない場合は最後の要素が小さくなる)\n",
    "print(\"# chunkで3グループに分割:\")\n",
    "print(b)\n",
    "print([c.size() for c in b])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 連結"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.cat`は既存のdimに沿ってテンソルを連結します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "torch.Size([2, 4])\n",
      "\n",
      "# torch.catでdim1方向に連結:\n",
      "tensor([[0, 1, 2, 0, 1, 2, 3],\n",
      "        [3, 4, 5, 4, 5, 6, 7]])\n",
      "torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(8).reshape(2, 4)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "c = torch.cat([a, b], dim=1) # 既存のdimで連結する. 他のdimのsizeは揃っている必要がある (numpy.concatenate)\n",
    "print(\"# torch.catでdim1方向に連結:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.stack`は新しいdimを作成し、そのdimに沿ってテンソルを連結します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "\n",
      "# torch.stackで新しい次元（dim2）方向に連結:\n",
      "tensor([[[0, 0],\n",
      "         [1, 1],\n",
      "         [2, 2]],\n",
      "\n",
      "        [[3, 3],\n",
      "         [4, 4],\n",
      "         [5, 5]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print(a.size())\n",
    "print()\n",
    "\n",
    "c = torch.stack([a, b], dim=2) # 新しいdimで連結する. 既存のdimのsizeはすべて揃っている必要がある (numpy.stack)\n",
    "print(\"# torch.stackで新しい次元（dim2）方向に連結:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 集約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.gather`は指定したdimに沿って対象の行列(input)のindexの要素を集約し、indexと同じサイズの行列を出力します。\n",
    "\n",
    "indexはinputと同じ次元数である必要があります。\n",
    "\n",
    "2次元の行列(input)に対する出力(out)は以下のようになります。\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0のとき\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1のとき\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "torch.Size([1, 3])\n",
      "# torch.gatherでdim0方向に関しては[0,1,0]で集める:\n",
      "tensor([[3, 1, 5]])\n",
      "torch.Size([1, 3])\n",
      "\n",
      "torch.Size([2, 2])\n",
      "# torch.gatherでdim1方向に関しては[0,1][1,0]で集める:\n",
      "tensor([[1, 0],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# dim=0の場合、第0軸方向をindexに沿って集約、その他の軸方向については順番に集約する\n",
    "index = torch.tensor([[1, 0, 1]])\n",
    "print(index.size())\n",
    "# この場合、第0軸方向は[1, 0, 1]、第1軸方向は[0, 1, 2]と集約されるので、[[a[1, 0], a[0, 1], a[1, 2]]]が出力される\n",
    "b = torch.gather(a, dim=0, index=index)\n",
    "print(\"# torch.gatherでdim0方向に関しては[0,1,0]で集める:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# dim=1の場合、第1軸方向をindexに沿って集約、その他の軸方向については順番に集約する\n",
    "index = torch.tensor([[1, 0], [0, 1]])\n",
    "print(index.size())\n",
    "# この場合、第0軸方向は[[0, 0], [1, 1]]、第1軸方向は[[1, 0], [0, 1]]と集約されるので、[[a[0, 1], a[0, 0]], [a[1, 0], a[1, 1]]が出力される\n",
    "b = torch.gather(a, dim=1, index=index)\n",
    "print(\"# torch.gatherでdim1方向に関しては[0,1][1,0]で集める:\")\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.index_select`も指定したdimに沿って対象の行列(input)のindexの要素を集約しますが、numpyのindexingに似た挙動です。\n",
    "\n",
    "indexはinputと同じ次元数である必要はなく、出力もindexと異なるサイズの出力となりえます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "torch.Size([3])\n",
      "# torch.index_selectでdim0方向に関して[1,0,1]で要素（行）を集める:\n",
      "tensor([[3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "torch.Size([5])\n",
      "# torch.index_selectでdim1方向に関して[0,1,2,1,0]で要素（列）を集める:\n",
      "tensor([[0, 1, 2, 1, 0],\n",
      "        [3, 4, 5, 4, 3]])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# dim=0\n",
    "index = torch.tensor([1, 0, 1])\n",
    "print(index.size())\n",
    "b = torch.index_select(a, dim=0, index=index)  # a[[1, 0, 1], :]\n",
    "print(\"# torch.index_selectでdim0方向に関して[1,0,1]で要素（行）を集める:\")\n",
    "print(b)\n",
    "print(b.size())\n",
    "print()\n",
    "\n",
    "# dim=1\n",
    "index = torch.tensor([0, 1, 2, 1, 0])\n",
    "print(index.size())\n",
    "b = torch.index_select(a, dim=1, index=index)  # [:, [0, 1, 2, 1, 0]]\n",
    "print(\"# torch.index_selectでdim1方向に関して[0,1,2,1,0]で要素（列）を集める:\")\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.5 演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.1 スカラー演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpyと同様にスカラー演算を実装することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4070, 0.4100, 0.9684],\n",
      "        [0.4561, 0.9694, 0.5361]])\n",
      "tensor([[0.4867, 0.4169, 0.7073],\n",
      "        [0.2929, 0.9335, 0.4679]])\n",
      "\n",
      "# 足し算:\n",
      "tensor([[0.8937, 0.8268, 1.6757],\n",
      "        [0.7490, 1.9029, 1.0041]])\n",
      "\n",
      "# 引き算:\n",
      "tensor([[-0.0796, -0.0069,  0.2610],\n",
      "        [ 0.1632,  0.0358,  0.0682]])\n",
      "\n",
      "# 掛け算:\n",
      "tensor([[0.1981, 0.1709, 0.6850],\n",
      "        [0.1336, 0.9050, 0.2509]])\n",
      "\n",
      "# 割り算:\n",
      "tensor([[0.8363, 0.9834, 1.3690],\n",
      "        [1.5572, 1.0384, 1.1457]])\n",
      "\n",
      "# log:\n",
      "tensor([[-0.8989, -0.8917, -0.0321],\n",
      "        [-0.7850, -0.0311, -0.6234]])\n",
      "\n",
      "# exp:\n",
      "tensor([[1.5023, 1.5068, 2.6337],\n",
      "        [1.5779, 2.6363, 1.7093]])\n",
      "\n",
      "# ルート:\n",
      "tensor([[0.6380, 0.6403, 0.9841],\n",
      "        [0.6754, 0.9846, 0.7322]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2, 3))\n",
    "b = torch.rand((2, 3))\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "print(\"# 足し算:\")\n",
    "print(a + b)\n",
    "print()\n",
    "\n",
    "print(\"# 引き算:\")\n",
    "print(a - b)\n",
    "print()\n",
    "\n",
    "print(\"# 掛け算:\")\n",
    "print(a * b)\n",
    "print()\n",
    "\n",
    "print(\"# 割り算:\")\n",
    "print(a / b)\n",
    "print()\n",
    "\n",
    "print(\"# log:\")\n",
    "print(torch.log(a))\n",
    "print()\n",
    "\n",
    "print(\"# exp:\")\n",
    "print(torch.exp(a))\n",
    "print()\n",
    "\n",
    "print(\"# ルート:\")\n",
    "print(torch.sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.2 集約演算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "集約演算もnumpyと同様に実装できますが、集約する次元をaxisではなくdimで指定することに注意する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8., 9.]])\n",
      "\n",
      "# 合計(dim0):\n",
      "tensor([ 5.,  7.,  9., 11., 13.])\n",
      "\n",
      "# 平均(dim1):\n",
      "tensor([2., 7.])\n",
      "\n",
      "# 分散(全体):\n",
      "tensor(9.1667)\n",
      "\n",
      "# 標準偏差(dim -1, 最後の次元):\n",
      "tensor([1.5811, 1.5811])\n",
      "\n",
      "# 最大値、argmax(dim0):\n",
      "(tensor([5., 6., 7., 8., 9.]), tensor([1, 1, 1, 1, 1]))\n",
      "\n",
      "# 最大値:\n",
      "tensor(9.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10, dtype=torch.float32).reshape(2, 5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# 合計(dim0):\")\n",
    "print(torch.sum(a, dim=0)) # a.sum(0)でも可\n",
    "print()\n",
    "\n",
    "print(\"# 平均(dim1):\")\n",
    "print(torch.mean(a, dim=1))\n",
    "print()\n",
    "\n",
    "print(\"# 分散(全体):\")\n",
    "print(torch.var(a)) # dimを指定しない場合は全体に対して適用\n",
    "print()\n",
    "\n",
    "print(\"# 標準偏差(dim -1, 最後の次元):\")\n",
    "print(torch.std(a, dim=-1)) # dim=-1とすると最後の次元に対して適用\n",
    "print()\n",
    "\n",
    "print(\"# 最大値、argmax(dim0):\")\n",
    "print(torch.max(a, dim=0)) # torch.maxは, maxとargmaxの両方を返す(torch.minも同様)\n",
    "print()\n",
    "\n",
    "print(\"# 最大値:\")\n",
    "print(torch.max(a)) # dimを指定しない場合はmax(or min)のみ返す\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.3 $Lp$ノルム\n",
    "\n",
    "行列のノルムも、numpy同様に求めることができます。(numpy.linalg.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "    ||x||_p = \\left(x^p_1 + x^p_2 + \\ldots + x^p_N\\right)^{1/p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "tensor(2.2361)\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(3, dtype=torch.float)\n",
    "print(a)\n",
    "print(torch.norm(a, p=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.5.4 行列・テンソル積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "行列の積もnumpy同様にdot, matmulで実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# torch.dot（ベクトルの内積）:\n",
      "tensor(4.)\n",
      "\n",
      "# torch.matmul（行列積）:\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4)\n",
    "b = torch.ones(4)\n",
    "\n",
    "c = torch.dot(a, b)\n",
    "print(\"# torch.dot（ベクトルの内積）:\")\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "b = torch.ones((3, 4))\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "print(\"# torch.matmul（行列積）:\")\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3階のテンソルの内積計算には`torch.bmm(batch1, batch2)`が使えます。\n",
    "\n",
    "batch1とbatch2は3階のテンソルで、それぞれが(b, n, m)と(b, m, p)のsizeを有する場合に(b, n, p)のテンソルを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# torch.bmm（batch matrix matrix product）:\n",
      "tensor([[[4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4.]],\n",
      "\n",
      "        [[4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4.],\n",
      "         [4., 4., 4., 4., 4.]]])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, 4)\n",
    "b = torch.ones(2, 4, 5)\n",
    "\n",
    "print(\"# torch.bmm（batch matrix matrix product）:\")\n",
    "c = torch.bmm(a, b)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "3階以上のテンソルを含む計算には`np.einsum`と同様にアインシュタインの縮約記法を用いる`einsum`での実装が便利です。\n",
    "\n",
    "einsumを用いると、テンソル積の計算を行う行列のサイズを添字で表現し、計算を行う前と後のサイズを与えて計算を行うことができます。\n",
    "\n",
    "詳しくはドキュメント（ https://pytorch.org/docs/stable/torch.html#torch.einsum ）を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n",
      "tensor(24.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3, 4))\n",
    "b = torch.ones((2, 3))\n",
    "\n",
    "c = torch.einsum('ijk,ij->k', (a, b))\n",
    "print(c)\n",
    "\n",
    "sum_c = torch.einsum('ijk,ij->', (a, b))\n",
    "print(sum_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.5.5 条件演算子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "時系列問題(RNNなど)でmaskを作るときなどに有効です.\n",
    "```\n",
    "torch.where(条件式, Trueの場合, Falseの場合)\n",
    "```\n",
    "のように書きます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4219, -0.2314,  0.3739],\n",
      "        [-0.4679, -0.7236,  0.2429]])\n",
      "\n",
      "# torch.whereで0以下のmask:\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "a = torch.randn((2, 3))\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# torch.whereで0以下のmask:\")\n",
    "masked_a = torch.where(a > 0, torch.ones_like(a), torch.zeros_like(a))\n",
    "print(masked_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpy.clipのように、値を一定の範囲でクリッピングします. logなどでのアンダーフローを防ぐときなどに有効です."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4219, -0.2314,  0.3739],\n",
      "        [-0.4679, -0.7236,  0.2429]])\n",
      "\n",
      "# torch.clampでクリッピング:\n",
      "tensor([[1.4219e+00, 1.0000e-10, 3.7390e-01],\n",
      "        [1.0000e-10, 1.0000e-10, 2.4287e-01]])\n",
      "tensor([[  0.3520, -23.0259,  -0.9838],\n",
      "        [-23.0259, -23.0259,  -1.4152]])\n",
      "\n",
      "# クリッピングしないとnanがでる:\n",
      "tensor([[ 0.3520,     nan, -0.9838],\n",
      "        [    nan,     nan, -1.4152]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(34)\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "x_clipped = torch.clamp(x, 1e-10, 1e+10)\n",
    "print(\"# torch.clampでクリッピング:\")\n",
    "print(x_clipped)\n",
    "print(torch.log(x_clipped))\n",
    "print()\n",
    "\n",
    "print(\"# クリッピングしないとnanがでる:\")\n",
    "print(torch.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.5.6 比較演算子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### torch.ge, torch.gt, torch.le, torch.lt, torch.eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "値の大小の比較を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "# a >= 3:\n",
      "tensor([0, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "\n",
      "# a > 3:\n",
      "tensor([0, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "\n",
      "# a <= 3:\n",
      "tensor([1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "\n",
      "# a < 3:\n",
      "tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "\n",
      "# a == 3:\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "print(\"# a >= 3:\")\n",
    "print(torch.ge(a, 3))  # greater than or equal to\n",
    "print(a >= 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a > 3:\")\n",
    "print(torch.gt(a, 3))  # greater than\n",
    "print(a > 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a <= 3:\")\n",
    "print(torch.le(a, 3))  # less than or equal to\n",
    "print(a <= 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a < 3:\")\n",
    "print(torch.lt(a, 3))  # less than\n",
    "print(a < 3)  # pythonの比較演算子を用いることも可能\n",
    "print()\n",
    "\n",
    "print(\"# a == 3:\")\n",
    "print(torch.eq(a, 3))  # equal to\n",
    "print(a == 3)  # pythonの比較演算子を用いることも可能\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.6 Numpy, list, scalarへの変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "numpy, list, scalarへの変換は以下のようにして行います."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "# a.numpy():\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "# a.tolist():\n",
      "[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n",
      "\n",
      "# a.item():\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3))\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# numpy.ndarrayへの変換\n",
    "print(\"# a.numpy():\")\n",
    "print(a.numpy())\n",
    "print()\n",
    "\n",
    "# listへの変換\n",
    "print(\"# a.tolist():\")\n",
    "print(a.tolist())\n",
    "print()\n",
    "\n",
    "# scalarへの変換には.item()を使います\n",
    "print(\"# a.item():\")\n",
    "print(a.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 デバイス(CPU/CUDA)の指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorを扱うデバイスを指定するには、`device`引数や`.to()`メソッドなどを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b644ff5cc267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# GPUへの移動(すべて同じです)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         raise RuntimeError(\n\u001b[1;32m    160\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1)\n",
    "print(a)\n",
    "print()\n",
    "\n",
    "# GPUへの移動(すべて同じです)\n",
    "b = a.cuda()\n",
    "print(b)\n",
    "b = a.to('cuda')\n",
    "print(b)\n",
    "b = torch.ones(1, device='cuda')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# CPUへの移動(すべて同じです)\n",
    "c = b.cpu()\n",
    "print(c)\n",
    "c = b.to('cpu')  \n",
    "print(c)\n",
    "c = torch.ones(1, device='cpu')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDAが使えるかどうかは`torch.cuda.is_available()`でわかるので、あらかじめこれでdeviceを取得しておくと便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.ones(1, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PyTorchでは、Tensorの自動微分をサポートするautogradの機能が提供されております。\n",
    "\n",
    "autogradの仕組みを理解する上で`Tensor`に加えて重要なクラスとして`Function`があります。\n",
    "\n",
    "FunctionはTensorを入力としてTensorを出力する関数であり、Tensorをノード、Functionをエッジとして計算グラフが構築されます。\n",
    "\n",
    "各Tensorは`.grad_fn`という属性を有しており、これはそのTensorを作成したFunctionを参照しています。（ユーザが自分で作成したTensorのgrad_fnはNoneになります）\n",
    "<img src=\"../figures/tensor.png\" width=\"400mm\">\n",
    "\n",
    "\n",
    "autogradを用いるには、まず、計算グラフを構築した後、forward関数によって入力のTensorから出力のTensorに対する順伝播の計算を行います。\n",
    "\n",
    "その後、backward関数を呼ぶことにより、`requires_grad=True`を指定したすべてのTensorの目的関数に関する勾配が計算されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 順伝播の計算\n",
    "x = torch.randn(4, 4)\n",
    "y = torch.randn(4, 1)\n",
    "\n",
    "w = torch.randn(4, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "\n",
    "# 目的関数の定義\n",
    "loss = (y_pred - y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "\n",
      "<AddBackward0 object at 0x7f1b41250668>\n"
     ]
    }
   ],
   "source": [
    "# ユーザが作成したTensorはgrad_fn=None\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "print(w.grad_fn)\n",
    "print(b.grad_fn)\n",
    "print()\n",
    "\n",
    "# Functionによって計算されたTensorはgrad_fnを有する\n",
    "print(y_pred.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# まだ勾配は計算されていない\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 逆伝播\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([[-11.0641],\n",
      "        [-11.6715],\n",
      "        [ 13.1470],\n",
      "        [ -6.0148]])\n",
      "tensor([6.9879])\n"
     ]
    }
   ],
   "source": [
    "# requires_grad=Trueを指定した変数は勾配が計算されている\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`.detach()`を使うことにより、Tensorの勾配計算を行わないようにすることもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  6.7204],\n",
      "        [-10.1176],\n",
      "        [ 12.7670],\n",
      "        [  8.8713]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = torch.randn(4, 1)\n",
    "\n",
    "w = torch.randn(4, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "b = b.detach()  # bの勾配計算を停止\n",
    "\n",
    "y_pred = torch.matmul(x, w) + b\n",
    "\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad)  # 勾配を有する\n",
    "print(b.grad)  # 勾配を有さない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "また、`with torch.no_grad():`でくくることで、その下で定義したTensorの勾配計算をまとめて停止させることが可能です。\n",
    "\n",
    "これは、学習済みのモデルを評価する際に、モデルが`requires_grad=True`となっているパラメータを有する場合でも勾配計算を行わないようにしたいときなどに有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad of y_pred: True\n",
      "requires_grad of y_eval: False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = torch.matmul(x, w) + b  # y_predと同様の計算を行う\n",
    "\n",
    "print('requires_grad of y_pred:', y_pred.requires_grad)  # requires_grad=True\n",
    "print('requires_grad of y_eval:', y_eval.requires_grad)  # requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "autogradは自動微分を可能にする強力な機能ですが、巨大なニューラルネットワークを低レベルのautogradのみで実装するのは大変です。\n",
    "\n",
    "ニューラルネットワークを構築する際には、学習可能なパラメータを有するいくつかのレイヤーを定義することが一般的ですが、Pytorchにはこれを行う上で`nn`という便利な高レベルのパッケージが存在します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.1 Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`nn`には、ニューラルネットワークにおけるレイヤーのような役目を果たす`Module`が実装されています。\n",
    "\n",
    "Tensorを入力としてTensorを出力しますが、学習可能なパラメータなどの内部状態を有し、forward関数とbackward関数を有します。\n",
    "\n",
    "線形層や畳み込み層など、一般的なレイヤーは一通り実装されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# 線形層\n",
    "nn.Linear(input_dim, output_dim)\n",
    "\n",
    "# 畳み込み層\n",
    "nn.Conv1d(input_dim, output_dim, kernel_size)\n",
    "\n",
    "# LSTM\n",
    "nn.LSTM(input_dim, hidden_dim, num_layers)\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.2 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "torch.nn.functionalに一般的な活性化関数は一通り揃っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4219, -0.2314,  0.3739],\n",
      "        [-0.4679, -0.7236,  0.2429]])\n",
      "\n",
      "tensor([[0.8056, 0.4424, 0.5924],\n",
      "        [0.3851, 0.3266, 0.5604]])\n",
      "\n",
      "tensor([[1.4219, 0.0000, 0.3739],\n",
      "        [0.0000, 0.0000, 0.2429]])\n",
      "\n",
      "tensor([[ 0.8900, -0.2274,  0.3574],\n",
      "        [-0.4365, -0.6191,  0.2382]])\n",
      "\n",
      "tensor([[ 1.4219, -0.0463,  0.3739],\n",
      "        [-0.0936, -0.1447,  0.2429]])\n",
      "\n",
      "tensor([[1.6380, 0.5841, 0.8975],\n",
      "        [0.4863, 0.3954, 0.8219]])\n",
      "\n",
      "tensor([[0.6485, 0.1241, 0.2274],\n",
      "        [0.2625, 0.2033, 0.5343]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(34)\n",
    "\n",
    "x = torch.randn((2, 3))\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(torch.sigmoid(x)) # F.sigmoidでもよいが、deprecatedなのでこちらが推奨されている\n",
    "print()\n",
    "\n",
    "print(F.relu(x))\n",
    "print()\n",
    "\n",
    "print(torch.tanh(x)) # F.tanhでもよいが、deprecatedなのでこちらが推奨されている\n",
    "print()\n",
    "\n",
    "print(F.leaky_relu(x, 0.2))\n",
    "print()\n",
    "\n",
    "print(F.softplus(x))\n",
    "print()\n",
    "\n",
    "print(F.softmax(x, dim=-1)) # 正規化したいdimを指定する. 最後のdimに対して行いたいときはdim=-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.3 誤差関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "一般的な誤差関数も一通り揃っています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# L1誤差\n",
    "nn.L1Loss()\n",
    "\n",
    "# 平均二乗誤差\n",
    "nn.MSELoss()\n",
    "\n",
    "# 交差エントロピー誤差\n",
    "nn.CrossEntropyLoss()\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.4 モジュール化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "自分で層・ネットワークなどをクラス化したい場合は, `nn.Module`のサブクラスとして作成し、`__init__`と`forward`をoverrideすることで実装できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):  # nn.Moduleを継承する\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):  # __init__をoverride\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):  # forwardをoverride\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "# feedforward：\n",
      "tensor([[0.5169],\n",
      "        [0.5112],\n",
      "        [0.4913],\n",
      "        [0.4899]], grad_fn=<SigmoidBackward>)\n",
      "\n",
      "# mlp.parameters()でモデルのパラメータ取得：\n",
      "<generator object Module.parameters at 0x7f1b412a8d00>\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(2, 3, 1)\n",
    "print(mlp)\n",
    "print()\n",
    "\n",
    "x = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = mlp(x) # forward(x)が呼ばれる\n",
    "print(\"# feedforward：\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "print(\"# mlp.parameters()でモデルのパラメータ取得：\")\n",
    "print(mlp.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.5 最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "torch.optimに一般的なoptimizerが実装されています。\n",
    "\n",
    "勾配のリセットは`.zero_grad()`で、パラメータの更新は`.step()`で行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# optimizerの定義\n",
    "optimizer = optim.SGD([W1, W2], lr=0.1)\n",
    "\n",
    "# 勾配のリセット\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# パラメータの更新\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4.6 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "これまで見てきたモジュールなどを用いてMLPのモデルを学習させる一連の流れを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7523696422576904\n",
      "100 0.6718162894248962\n",
      "200 0.6125582456588745\n",
      "300 0.5464031100273132\n",
      "400 0.5096918344497681\n",
      "500 0.4900974929332733\n",
      "600 0.4589649438858032\n",
      "700 0.3662275969982147\n",
      "800 0.25440165400505066\n",
      "900 0.17582100629806519\n"
     ]
    }
   ],
   "source": [
    "# XORをMLPで行う\n",
    "x = torch.Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "t = torch.Tensor([0, 1, 1, 0])\n",
    "\n",
    "# モデルの定義\n",
    "mlp = MLP(2, 3, 1)\n",
    "\n",
    "# 誤差関数の定義\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "# 最適化の定義\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.1)  # Moduleのパラメータは.parameters()で取得できる\n",
    "\n",
    "# モデルを訓練モードにする（Dropout等に関係）\n",
    "mlp.train()\n",
    "\n",
    "for i in range(1000):\n",
    "    # 順伝播\n",
    "    y_pred = mlp(x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    loss = criterion(y_pred, t.unsqueeze(1))\n",
    "    \n",
    "    # 逆伝播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの更新\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4.7 モデルの保存・読み込み・再学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "モデルを保存する際には、`torch.save()`を用いますが、モデルのインスタンスを直接保存するのではなく、モデルのパラメータの情報を有するstate_dictを保存し、読み込む際にもstate_dictを読み込んでモデルのインスタンスにloadするのが一般的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.5302,  0.3653],\n",
      "        [ 1.5943,  1.6213],\n",
      "        [ 2.3035,  2.3063]], requires_grad=True), Parameter containing:\n",
      "tensor([-3.6619e-01, -1.3848e-03, -2.2977e+00], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2788,  2.2241, -3.8807]], requires_grad=True), Parameter containing:\n",
      "tensor([-1.1215], requires_grad=True)]\n",
      "\n",
      "OrderedDict([('linear1.weight', tensor([[-0.5302,  0.3653],\n",
      "        [ 1.5943,  1.6213],\n",
      "        [ 2.3035,  2.3063]])), ('linear1.bias', tensor([-3.6619e-01, -1.3848e-03, -2.2977e+00])), ('linear2.weight', tensor([[-0.2788,  2.2241, -3.8807]])), ('linear2.bias', tensor([-1.1215]))])\n"
     ]
    }
   ],
   "source": [
    "print(list(mlp.parameters()))\n",
    "print()\n",
    "\n",
    "# state_dictの取得\n",
    "state_dict = mlp.state_dict()\n",
    "print(state_dict)\n",
    "\n",
    "# モデルの保存\n",
    "torch.save(state_dict, './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1315, -0.1273],\n",
      "        [ 0.6624, -0.0621],\n",
      "        [ 0.6638,  0.0511]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0189, -0.1175,  0.2932], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2391,  0.5006, -0.0370]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3289], requires_grad=True)]\n",
      "\n",
      "[Parameter containing:\n",
      "tensor([[-0.5302,  0.3653],\n",
      "        [ 1.5943,  1.6213],\n",
      "        [ 2.3035,  2.3063]], requires_grad=True), Parameter containing:\n",
      "tensor([-3.6619e-01, -1.3848e-03, -2.2977e+00], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2788,  2.2241, -3.8807]], requires_grad=True), Parameter containing:\n",
      "tensor([-1.1215], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "mlp2 = MLP(2, 3, 1)\n",
    "print(list(mlp2.parameters()))  # ランダムな初期値\n",
    "print()\n",
    "\n",
    "# 学習済みパラメータの読み込み\n",
    "state_dict = torch.load('./model.pth')\n",
    "mlp2.load_state_dict(state_dict)\n",
    "print(list(mlp2.parameters()))  # 学習済みパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tips: torchvision & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchvisionは画像認識のためのデータセットや前処理、学習済みモデルなどが収められたpytorch公式のライブラリです。\n",
    "\n",
    "MNISTなどのよく使われるデータセットを`torchvision.datasets`から簡単に読み込めるほか、前処理は`transform`としてまとめて`torch.utils.data.DataLoader`に渡すだけでOKです。\n",
    "\n",
    "MNISTを使用する際の例を以下に示します."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(in_dim))\n",
    "])\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data/mnist', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 課題：PyTorchを使ってMLPを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.linear2 = nn.Linear(hid_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.log_softmax(self.linear2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim  = 784\n",
    "hid_dim = 200\n",
    "out_dim = 10\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "\n",
    "mlp = MLP(in_dim, hid_dim, out_dim).to(device)\n",
    "\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()  # Negative Log Liklihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データローダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 前処理を定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(in_dim))\n",
    "])\n",
    "\n",
    "# torchvisionのdatasetsを使ってMNISTのデータを取得\n",
    "# ミニバッチ化や前処理などの処理を行ってくれるDataLoaderを定義\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/mnist', train=False, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Train [Loss: 2.105, F1: 0.481], Valid [Loss: 1.845, F1: 0.671]\n",
      "EPOCH: 1, Train [Loss: 1.543, F1: 0.708], Valid [Loss: 1.230, F1: 0.761]\n",
      "EPOCH: 2, Train [Loss: 1.044, F1: 0.791], Valid [Loss: 0.862, F1: 0.821]\n",
      "EPOCH: 3, Train [Loss: 0.784, F1: 0.831], Valid [Loss: 0.682, F1: 0.852]\n",
      "EPOCH: 4, Train [Loss: 0.649, F1: 0.850], Valid [Loss: 0.582, F1: 0.863]\n",
      "EPOCH: 5, Train [Loss: 0.569, F1: 0.863], Valid [Loss: 0.518, F1: 0.875]\n",
      "EPOCH: 6, Train [Loss: 0.517, F1: 0.870], Valid [Loss: 0.476, F1: 0.883]\n",
      "EPOCH: 7, Train [Loss: 0.480, F1: 0.877], Valid [Loss: 0.444, F1: 0.887]\n",
      "EPOCH: 8, Train [Loss: 0.452, F1: 0.882], Valid [Loss: 0.421, F1: 0.890]\n",
      "EPOCH: 9, Train [Loss: 0.431, F1: 0.885], Valid [Loss: 0.402, F1: 0.894]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    preds_train = []\n",
    "    preds_valid = []\n",
    "    trues_train = []\n",
    "    trues_valid = []\n",
    "    \n",
    "    mlp.train()\n",
    "    for x, t in dataloader_train:\n",
    "        true = t.tolist()\n",
    "        trues_train.extend(true)\n",
    "\n",
    "        # 勾配の初期化\n",
    "        mlp.zero_grad()\n",
    "        \n",
    "        # テンソルをGPUに移動\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # 順伝播\n",
    "        y = mlp.forward(x)\n",
    "        \n",
    "        # 誤差の計算\n",
    "        loss = criterion(y, t)\n",
    "        \n",
    "        # 誤差の逆伝播\n",
    "        loss.backward()\n",
    "        \n",
    "        # パラメータの更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        # モデルの出力を予測値のスカラーに変換\n",
    "        pred = y.argmax(1).tolist()\n",
    "        preds_train.extend(pred)\n",
    "        \n",
    "        losses_train.append(loss.tolist())\n",
    "    \n",
    "    mlp.eval()\n",
    "    for x, t in dataloader_valid:\n",
    "        true = t.tolist()\n",
    "        trues_valid.extend(true)\n",
    "\n",
    "        # テンソルをGPUに移動\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        \n",
    "        # 順伝播\n",
    "        y = mlp.forward(x)\n",
    "\n",
    "        # 誤差の計算\n",
    "        loss = criterion(y, t)\n",
    "        \n",
    "        # モデルの出力を予測値のスカラーに変換\n",
    "        pred = y.argmax(1).tolist()\n",
    "        preds_valid.extend(pred)\n",
    "        \n",
    "        losses_valid.append(loss.tolist())\n",
    "        \n",
    "    print('EPOCH: {}, Train [Loss: {:.3f}, F1: {:.3f}], Valid [Loss: {:.3f}, F1: {:.3f}]'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        f1_score(trues_train, preds_train, average='macro'),\n",
    "        np.mean(losses_valid),\n",
    "        f1_score(trues_valid, preds_valid, average='macro')\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
